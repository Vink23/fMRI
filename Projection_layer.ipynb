{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPiuUbFqIhE+OgUlp6H/zDU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EfVMLo8JijD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce391b1-476f-474b-c6b4-b1e6bf78ad42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/491.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torchinfo\n",
        "!pip install -U -q datasets fsspec huggingface_hub # Hugging Face's dataset library\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from torchinfo import summary\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification,AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import random\n",
        "from itertools import islice\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CosineSimilarity\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.nn import MSELoss, CosineSimilarity\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, IterableDataset"
      ],
      "metadata": {
        "id": "J21cErYljOWp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RGS6ZY33qzYt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For sentence processing later\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmfuXCWt42wt",
        "outputId": "82a77c3e-f427-4102-a72a-a603ddc0bdd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "unV_dQEin6PW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    llama_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name)\n",
        "    llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
        "    bert_model.eval()\n",
        "    llama_model.eval()\n",
        "    return bert_model, bert_tokenizer, llama_model, llama_tokenizer"
      ],
      "metadata": {
        "id": "5MQQiySKfiPf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert, bert_tokenizer, llama, llama_tokenizer = load_models()"
      ],
      "metadata": {
        "id": "sW0l0-hKm3gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bf5884-8aeb-44ad-99c9-e93727350692"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, tokenizer, max_len=20):\n",
        "    return tokenizer(\n",
        "        sentence,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )"
      ],
      "metadata": {
        "id": "ggVthfDnf9r6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llama_embed_dim = llama.get_input_embeddings().weight.size(-1)\n",
        "# llama_embed_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY55jsoOqNnI",
        "outputId": "83a811be-fd42-40c2-b54d-c6588a5a97b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_projection_layer(\n",
        "    bert, llama, projection, data_stream,\n",
        "    tokenizer_bert, tokenizer_llama,\n",
        "    optimizer, loss_fn, max_steps=5000,\n",
        "    log_interval=50, checkpoint_interval=1000, device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = next(projection.parameters()).device\n",
        "    step = 0\n",
        "    running_loss = 0.0\n",
        "    running_cosine = 0.0\n",
        "    cosine_fn = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    for example in data_stream:\n",
        "        text = example.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Get first sentence\n",
        "        sentence = sent_tokenize(text)[0] if len(sent_tokenize(text)) > 0 else \"\"\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        # Encode with BERT\n",
        "        bert_inputs = {k: v.to(device) for k, v in\n",
        "               encode_sentence(sentence, tokenizer_bert).items()}\n",
        "        with torch.no_grad():\n",
        "            output = bert(**bert_inputs)\n",
        "            token_embeddings = output.last_hidden_state\n",
        "            attention_mask = bert_inputs['attention_mask'].unsqueeze(-1)\n",
        "            masked_output = token_embeddings * attention_mask\n",
        "            pooled = masked_output.sum(dim=1) / attention_mask.sum(dim=1)\n",
        "            pooled = pooled.to(device)\n",
        "\n",
        "\n",
        "        # Encode with LLaMA\n",
        "        llama_inputs = {k: v.to(device)\n",
        "                for k, v in encode_sentence(sentence, tokenizer_llama).items()}\n",
        "\n",
        "        llama_input_ids  = llama_inputs[\"input_ids\"]\n",
        "        llama_input_embed = llama.get_input_embeddings()(llama_input_ids)\n",
        "\n",
        "        if llama_input_embed.shape[1] < 2:\n",
        "            continue\n",
        "\n",
        "        target_embedding = llama_input_embed[:, 1, :].detach()\n",
        "        projected = projection(pooled)\n",
        "\n",
        "        # Compute loss\n",
        "        mse_loss = loss_fn(projected, target_embedding)\n",
        "        cosine_sim = cosine_fn(projected, target_embedding)\n",
        "        loss = mse_loss - 0.5 * cosine_sim.mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        cosine_sim = cosine_fn(projected, target_embedding).mean().item()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item()\n",
        "        running_cosine += cosine_sim\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            avg_loss = running_loss / (log_interval if step > 0 else 1)\n",
        "            avg_cos = running_cosine / (log_interval if step > 0 else 1)\n",
        "            print(f\"Step {step}: Avg MSE = {avg_loss:.4f} | Avg Cosine = {avg_cos:.4f}\")\n",
        "            running_loss = 0.0\n",
        "            running_cosine = 0.0\n",
        "\n",
        "        if step > 0 and step % checkpoint_interval == 0:\n",
        "            print(\"Saving model checkpoint...\")\n",
        "            torch.save({\n",
        "                'step': step,\n",
        "                'model_state_dict': projection.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss.item(),\n",
        "                'cosine_similarity': cosine_sim,\n",
        "            }, f\"projection_checkpoint_step_{step}.pth\")\n",
        "\n",
        "        step += 1\n",
        "        if step >= max_steps:\n",
        "            break"
      ],
      "metadata": {
        "id": "8ITZoJOqfutE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_prompt(prompt, bert, llama, projection,\n",
        "                              tokenizer_bert, tokenizer_llama,\n",
        "                              max_new_tokens=50, top_p=0.9, temperature=1.0):\n",
        "\n",
        "    device = next(bert.parameters()).device\n",
        "    bert_inputs = {k: v.to(device) for k, v in\n",
        "                   encode_sentence(prompt, tokenizer_bert).items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bert_output = bert(**bert_inputs)\n",
        "        token_emb   = bert_output.last_hidden_state\n",
        "        mask        = bert_inputs['attention_mask'].unsqueeze(-1)\n",
        "        pooled      = (token_emb * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "        projected   = projection(pooled)\n",
        "\n",
        "    input_ids = tokenizer_llama(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "\n",
        "\n",
        "    input_emb = llama.get_input_embeddings()(input_ids)\n",
        "    if input_emb.shape[1] >= 2:\n",
        "        input_emb[:, 1, :] = projected\n",
        "    else:\n",
        "        print(\"Warning: Prompt too short, skipping token replacement.\")\n",
        "\n",
        "    # Step 3: Generate text using top-p sampling\n",
        "    with torch.no_grad():\n",
        "        outputs = llama.generate(\n",
        "            inputs_embeds=input_emb,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            eos_token_id=tokenizer_llama.eos_token_id,\n",
        "            pad_token_id=tokenizer_llama.pad_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer_llama.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\n Prompt: {prompt}\\n Generated: {generated_text[len(prompt):].strip()}\\n\")\n"
      ],
      "metadata": {
        "id": "VP4SAMI-0WTg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_cosine(llama, llama_tokenizer, device, n_sent=1000):\n",
        "    embed_dim = llama.get_input_embeddings().weight.size(-1)\n",
        "    eye_proj  = nn.Linear(embed_dim, embed_dim, bias=False).to(device)\n",
        "    with torch.no_grad(): eye_proj.weight.copy_(torch.eye(embed_dim, device=device))\n",
        "    dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
        "    cos_fn = nn.CosineSimilarity(dim=-1); total, k = 0., 0\n",
        "    llama.eval()\n",
        "    for ex in dataset.take(n_sent):\n",
        "        sent = ex[\"text\"].split(\".\")[0]\n",
        "        ids  = llama_tokenizer(sent, return_tensors=\"pt\").input_ids.to(device)\n",
        "        tgt  = llama.get_input_embeddings()(ids)[:, 1, :]\n",
        "        out  = eye_proj(tgt)\n",
        "        cos  = cos_fn(F.normalize(out,-1), F.normalize(tgt,-1)).mean().item()\n",
        "        total += cos; k += 1\n",
        "    print(f\"[Identity check] Avg cosine ≈ {total/k:.4f}\")\n",
        "\n",
        "\n",
        "def procrustes_rotation(dataset, num_pairs, bert, bert_tok,\n",
        "                         llama, llama_tok, device, hidden_out):\n",
        "    X, Y = [], []\n",
        "    with torch.no_grad():\n",
        "        for ex in dataset.take(num_pairs):\n",
        "            sent = ex[\"text\"].split(\".\")[0]\n",
        "            # BERT pooled\n",
        "            b_ids = bert_tok(sent, return_tensors=\"pt\").to(device)\n",
        "            b_out = bert(**b_ids).last_hidden_state\n",
        "            mask  = b_ids[\"attention_mask\"].unsqueeze(-1)\n",
        "            pooled = (b_out*mask).sum(1)/mask.sum(1)              # (1,768)\n",
        "            # Llama token‑1 embed\n",
        "            l_ids = llama_tok(sent, return_tensors=\"pt\").input_ids.to(device)\n",
        "            l_emb = llama.get_input_embeddings()(l_ids)[:,1,:]    # (1,4096)\n",
        "            X.append(pooled.squeeze().cpu().numpy())\n",
        "            Y.append(l_emb.squeeze().cpu().numpy())\n",
        "    X, Y = np.stack(X), np.stack(Y)\n",
        "\n",
        "    Xp, Yp = PCA(256).fit_transform(X), PCA(256).fit_transform(Y)\n",
        "    U, _, Vt = np.linalg.svd(Yp.T @ Xp)\n",
        "    R = (U @ Vt)[:hidden_out, :]                                # slice if needed\n",
        "    return torch.tensor(R, dtype=torch.float32)\n",
        "\n",
        "def load_rotation_into_projection(R, projection):\n",
        "    with torch.no_grad():\n",
        "        projection[0].weight.copy_(R)\n",
        "\n",
        "\n",
        "def plot_cosine_vs_len(n_samples, bert, bert_tok, llama, llama_tok,\n",
        "                       projection, device):\n",
        "    buckets = defaultdict(list); cos_fn = nn.CosineSimilarity(dim=-1)\n",
        "    with torch.no_grad():\n",
        "        for ex in dataset.take(n_samples):\n",
        "            sent    = ex[\"text\"].split(\".\")[0]\n",
        "            length  = len(sent.split())\n",
        "            # embeddings\n",
        "            b_ids = bert_tok(sent, return_tensors=\"pt\").to(device)\n",
        "            b_out = bert(**b_ids).last_hidden_state\n",
        "            mask  = b_ids[\"attention_mask\"].unsqueeze(-1)\n",
        "            pooled = (b_out*mask).sum(1)/mask.sum(1)\n",
        "            proj   = projection(pooled)\n",
        "            l_ids  = llama_tok(sent, return_tensors=\"pt\").input_ids.to(device)\n",
        "            tgt    = llama.get_input_embeddings()(l_ids)[:,1,:]\n",
        "            cosine = cos_fn(F.normalize(proj,-1), F.normalize(tgt,-1)).item()\n",
        "            buckets[length].append(cosine)\n",
        "    xs = sorted(buckets); ys = [np.mean(buckets[k]) for k in xs]\n",
        "    plt.plot(xs, ys, marker=\"o\"); plt.xlabel(\"# tokens\"); plt.ylabel(\"cosine\")\n",
        "    plt.title(\"Cosine vs prompt length\"); plt.show()\n",
        "\n",
        "\n",
        "def alpha_grid(alphas, steps, train_fn):\n",
        "    results = {}\n",
        "    for a in alphas:\n",
        "        loss_fn = lambda p,t: F.mse_loss(p,t) - a*F.cosine_similarity(p,t,dim=1).mean()\n",
        "        val_cos = train_fn(loss_fn, steps)\n",
        "        results[a] = val_cos\n",
        "    print(\"α‑grid results:\", results)\n",
        "\n",
        "def batch_sweep(exp_list, build_loader_fn, train_epoch_fn, eval_fn):\n",
        "    logs = []\n",
        "    for e in exp_list:\n",
        "        eff_bsz = 2**e\n",
        "        loader  = build_loader_fn(eff_bsz)\n",
        "        train_epoch_fn(loader)\n",
        "        logs.append((math.log2(eff_bsz), eval_fn()))\n",
        "    xs, ys = zip(*logs)\n",
        "    plt.plot(xs, ys, marker=\"x\"); plt.xlabel(\"log2(batch)\"); plt.ylabel(\"cosine\")\n",
        "    plt.title(\"Cosine vs batch size\"); plt.show()\n"
      ],
      "metadata": {
        "id": "lpNY3N1qxQpF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    set_seed()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    bert,  bert_tokenizer,  llama,  llama_tokenizer = load_models()\n",
        "    bert.to(device)\n",
        "    llama.to(device)\n",
        "\n",
        "\n",
        "    dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
        "\n",
        "\n",
        "    identity_cosine(llama, llama_tokenizer, device, n_sent=1000)\n",
        "\n",
        "    # Initialize projection layer and optimizer\n",
        "    llama_embed_dim = llama.get_input_embeddings().weight.size(-1)\n",
        "    hidden_1 = 2048\n",
        "\n",
        "    projection = nn.Sequential(\n",
        "        nn.LayerNorm(768),\n",
        "        nn.Linear(768, hidden_1),     # 768 → 2048\n",
        "        nn.ReLU(),\n",
        "        nn.LayerNorm(hidden_1),       # (→ 2048)\n",
        "\n",
        "        nn.Linear(hidden_1, llama_embed_dim),# 2048 → 4096\n",
        "        nn.ReLU(),\n",
        "        nn.LayerNorm(llama_embed_dim),       # (→ 4096)\n",
        "\n",
        "        nn.Linear(llama_embed_dim, llama_embed_dim)\n",
        "    ).to(device)\n",
        "    # projection = nn.Sequential(\n",
        "    #     nn.Linear(768, 2048),\n",
        "    #     nn.ReLU(),\n",
        "    #     nn.Linear(2048, llama_embed_dim),   # <- use llama_embed_dim here\n",
        "    #     nn.LayerNorm(llama_embed_dim)\n",
        "    # ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(projection.parameters(), lr=1e-6)\n",
        "\n",
        "\n",
        "    def combined_loss_fn(projected, target):\n",
        "        mse = MSELoss()(projected, target)\n",
        "        cosine = CosineSimilarity(dim=1)(projected, target).mean()\n",
        "        return mse - 0.9 * cosine\n",
        "\n",
        "    loss_fn = combined_loss_fn\n",
        "\n",
        "    # Train projection layer\n",
        "    train_projection_layer(\n",
        "        bert, llama, projection,\n",
        "        dataset, bert_tokenizer, llama_tokenizer,\n",
        "        optimizer, loss_fn,\n",
        "        max_steps=100000, log_interval=1000\n",
        "    )\n",
        "\n",
        "    # Save the trained projection layer\n",
        "    torch.save(projection.state_dict(), \"projection_layer.pt\")\n",
        "    print(\"Training complete. Projection layer saved to 'projection_layer.pt'.\")\n",
        "\n",
        "        # Generate text using the trained projection\n",
        "    test_prompts = [\n",
        "        \"The future of AI in medicine\",\n",
        "        \"Climate change will impact\",\n",
        "        \"In 10 years, robots will\",\n",
        "        \"Democracy and technology are\",\n",
        "        \"Humanity's next big leap is\"\n",
        "    ]\n",
        "    for prompt in test_prompts:\n",
        "        generate_text_from_prompt(\n",
        "            prompt,\n",
        "            bert, llama, projection,\n",
        "            bert_tokenizer, llama_tokenizer,\n",
        "            max_new_tokens=50,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Aam6R0fpfzeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa8f17f-e97d-4658-b3b1-3dcfbaf0b03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Identity check] Avg cosine ≈ 1.0000\n",
            "Step 0: Avg MSE = 0.3405 | Avg Cosine = -0.0038\n",
            "Step 1000: Avg MSE = 0.1464 | Avg Cosine = 0.0754\n",
            "Saving model checkpoint...\n",
            "Step 2000: Avg MSE = -0.0673 | Avg Cosine = 0.1709\n",
            "Saving model checkpoint...\n",
            "Step 3000: Avg MSE = -0.1585 | Avg Cosine = 0.2077\n",
            "Saving model checkpoint...\n",
            "Step 4000: Avg MSE = -0.2202 | Avg Cosine = 0.2317\n",
            "Saving model checkpoint...\n",
            "Step 5000: Avg MSE = -0.2472 | Avg Cosine = 0.2374\n",
            "Saving model checkpoint...\n",
            "Step 6000: Avg MSE = -0.2657 | Avg Cosine = 0.2417\n",
            "Saving model checkpoint...\n",
            "Step 7000: Avg MSE = -0.2833 | Avg Cosine = 0.2477\n",
            "Saving model checkpoint...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyfpTlhQh9rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUNGtaa3uVXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dW-FdEb8uVQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKL-fyH7uVON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paq23LvruVLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUN29oP0uVJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "le7QfYJ2h9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpxBElOD0J-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "em36DgRzln12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvanJkJJlnzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}