{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1rIFVOWSJ26UMv8D9a7bPiCAvuHs8aN8-",
      "authorship_tag": "ABX9TyN8bzRUmEQtriMeal/xcxgj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "!pip install -q transformers torchinfo datasets fsspec huggingface_hub evaluate xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn3IeBsvLnZo",
        "outputId": "ed0eb0ed-6018-4102-f3cf-bfb77b8ddb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Collecting torchvision==0.22.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio==2.7.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.22.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.22.0) (11.2.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchvision, torchaudio\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SUMqqSn4F3pd",
        "outputId": "fd8aaf73-b508-47ae-b4cf-60c5e2bca7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "301c58388ba74a1a9d24c3b802244ddf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixtjhl25FMB3",
        "outputId": "809619a5-6a63-444e-a8a4-ce849e6b039d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, random\n",
        "from transformers import (\n",
        "    AutoModel, AutoTokenizer,\n",
        "    AutoModelForCausalLM, BitsAndBytesConfig\n",
        ")\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import pathlib, torch, datetime as dt\n",
        "import xformers\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "CSQ1BbuWFF-d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_DIR = pathlib.Path(\"/content/drive/MyDrive/fMRIwork/checkpoints\")\n",
        "CKPT_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "oyIDCi6gqVIM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/fMRIwork/Projector_Training_Data/c4training_small.txt\"\n",
        "with open(path, encoding=\"utf-8\") as f:\n",
        "    corpus = [line.strip() for line in f]\n",
        "print(f\"Loaded {len(corpus):,} lines\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhcrTqtkFVuB",
        "outputId": "72ae6276-028c-42cc-dead-0fdeab32bacb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 50,000 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR5_YnzDPoeK",
        "outputId": "8b80d7a7-f87a-4ae0-d5b0-c019c18b860f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beginners BBQ Class Taking Place in Missoula!',\n",
              " \"Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\",\n",
              " 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.',\n",
              " 'How many backlinks per day for new site?',\n",
              " 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ CONFIG --------------------------------\n",
        "NEOBERT_CKPT   = \"chandar-lab/NeoBERT\" #\"bert-base-uncased\"\n",
        "LLAMA_CKPT     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "BATCH_SIZE     = 128 #64\n",
        "LR             = 2e-4\n",
        "NUM_EPOCHS     = 10\n",
        "MAX_LEN        = 64                             # truncate long sentences\n",
        "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED           = 42\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)"
      ],
      "metadata": {
        "id": "atirlASLFF3s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- Load models -----------------------------\n",
        "print(\"Loading NeoBERT …\")\n",
        "neo_tok  = AutoTokenizer.from_pretrained(NEOBERT_CKPT)\n",
        "neo_bert = AutoModel.from_pretrained(NEOBERT_CKPT,trust_remote_code=True).requires_grad_(False).to(DEVICE)\n",
        "\n",
        "print(\"Loading TinyLlama …\")\n",
        "bnb_cfg  = BitsAndBytesConfig(load_in_8bit=True)\n",
        "llm_tok  = AutoTokenizer.from_pretrained(LLAMA_CKPT)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA_CKPT,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,      # load weights in half-precision\n",
        ").eval().requires_grad_(False)\n",
        "\n",
        "\n",
        "bert_dim  = neo_bert.config.hidden_size                   # 768\n",
        "llama_dim = llm.config.hidden_size                        # 2048\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liwM5X5kFKcx",
        "outputId": "0b98b243-8b35-4b1c-bc05-593d3554f627"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading NeoBERT …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyLlama …\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ 2. Projector net -----------------------------\n",
        "class Bert2Llama(nn.Module):\n",
        "    \"\"\"4-layer GELU MLP: 768 ➜ 2048\"\"\"\n",
        "    def __init__(self, in_d=bert_dim, out_d=llama_dim, h_d=1536, n=5): #n=4):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(n):\n",
        "            layers.append(nn.Linear(in_d if i == 0 else h_d,\n",
        "                                    out_d if i == n-1 else h_d))\n",
        "            if i < n-1:\n",
        "                layers.append(nn.GELU())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):          # x: (B, T, 768)\n",
        "        return self.net(x)         #    (B, T, 2048)\n",
        "\n",
        "projector = Bert2Llama().to(DEVICE)"
      ],
      "metadata": {
        "id": "ZZm9YF5KFRG9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    # Encode with both tokenizers\n",
        "    neo_enc  = neo_tok(batch, padding=True, truncation=True,\n",
        "                       max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    llama_enc= llm_tok(batch, padding=True, truncation=True,\n",
        "                       max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "    return neo_enc, llama_enc\n",
        "\n",
        "loader = DataLoader(corpus, batch_size=BATCH_SIZE,\n",
        "                    shuffle=True, collate_fn=collate, num_workers=2)"
      ],
      "metadata": {
        "id": "7GSzIp--FXws"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Training loop -------------------------------\n",
        "optim = torch.optim.AdamW(projector.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=llm_tok.pad_token_id)\n",
        "global_step  = 0\n",
        "\n",
        "print(\"\\n===== Training projector =====\")\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    total, n_tok = 0.0, 0\n",
        "    for neo_enc, llama_enc in loader:\n",
        "\n",
        "        # Move to GPU\n",
        "        neo_enc  = {k:v.to(DEVICE) for k,v in neo_enc.items()}\n",
        "        llama_ids= llama_enc[\"input_ids\"].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            neo_out = neo_bert(**neo_enc).last_hidden_state  # (B,T,768)\n",
        "\n",
        "        # Align sequence lengths\n",
        "        T = min(neo_out.size(1), llama_ids.size(1))\n",
        "        neo_out   = neo_out[:, :T, :]                        # (B,T,768)\n",
        "        labels    = llama_ids[:, :T]                         # (B,T)\n",
        "\n",
        "        # Forward through projector & LLaMA\n",
        "        proj_emb = projector(neo_out)                  # (B, T, 2048)\n",
        "        proj_emb = proj_emb.to(dtype=next(llm.parameters()).dtype)\n",
        "        out = llm(inputs_embeds=proj_emb, labels=labels)\n",
        "\n",
        "        loss = out.loss\n",
        "        loss.backward(); optim.step(); optim.zero_grad()\n",
        "\n",
        "        global_step += 1\n",
        "        total  += loss.item() * labels.numel()\n",
        "        n_tok  += labels.numel()\n",
        "\n",
        "        if global_step % 1_000 == 0:\n",
        "            torch.save({\n",
        "                \"epoch\":      epoch,\n",
        "                \"step\":       global_step,\n",
        "                \"proj_state\": projector.state_dict(),\n",
        "                \"opt_state\":  optim.state_dict(),\n",
        "                \"rng_state\":  torch.get_rng_state(),\n",
        "                \"rng_state_cuda\": torch.cuda.get_rng_state_all()\n",
        "                                   if torch.cuda.is_available() else None,\n",
        "            }, CKPT_DIR / f\"step_{global_step:07d}.pt\")\n",
        "\n",
        "    # Epoch checkpoint\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"proj_state\": projector.state_dict(),\n",
        "            \"opt_state\":  optim.state_dict(),\n",
        "            \"rng_state\":  torch.get_rng_state(),\n",
        "        },\n",
        "        CKPT_DIR / f\"epoch_{epoch:02d}.pt\"\n",
        "    )\n",
        "\n",
        "    ppl = torch.exp(torch.tensor(total / n_tok))\n",
        "    print(f\"Epoch {epoch}: loss {total/n_tok:.4f}  |  ppl {ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN_1_KOVFaVq",
        "outputId": "5a352486-47f9-4fd0-8fd1-714731bb6936"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training projector ===\n",
            "Epoch 1: loss 4.6100  |  ppl 100.48\n",
            "Epoch 2: loss 3.2884  |  ppl 26.80\n",
            "Epoch 3: loss 2.9417  |  ppl 18.95\n",
            "Epoch 4: loss 2.7392  |  ppl 15.47\n",
            "Epoch 5: loss 2.5882  |  ppl 13.31\n",
            "Epoch 6: loss 2.4486  |  ppl 11.57\n",
            "Epoch 7: loss 2.3302  |  ppl 10.28\n",
            "Epoch 8: loss 2.2125  |  ppl 9.14\n",
            "Epoch 9: loss 2.1238  |  ppl 8.36\n",
            "Epoch 10: loss 2.0250  |  ppl 7.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wNZ2xS34E-Bj"
      },
      "outputs": [],
      "source": [
        "def neo2llama(text, max_new=0):\n",
        "    \"\"\"Feed NeoBERT → projector → LLaMA.\n",
        "       If max_new==0 → teacher forcing (should echo).\n",
        "       Else           → free generation (should still echo first part).\"\"\"\n",
        "    with torch.no_grad():\n",
        "        neo_in = neo_tok(text, return_tensors=\"pt\").to(DEVICE)\n",
        "        bert_h = neo_bert(**neo_in).last_hidden_state       # (1, B_len, 768)\n",
        "        bert_h = bert_h.to(dtype=projector.net[0].weight.dtype)\n",
        "        proj_h = projector(bert_h)\n",
        "\n",
        "        llama_in = llm_tok(text, return_tensors=\"pt\").to(DEVICE)\n",
        "        labels   = llama_in[\"input_ids\"]  # (1, L_len)\n",
        "\n",
        "\n",
        "        mask   = torch.ones(proj_h.size()[:2], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "\n",
        "        if max_new == 0:\n",
        "            T = min(proj_h.size(1), labels.size(1))\n",
        "            proj_h = proj_h[:, :T, :]\n",
        "            labels = labels[:, :T]\n",
        "\n",
        "\n",
        "            out    = llm(inputs_embeds=proj_h, labels=labels)\n",
        "            ids    = out.logits.argmax(-1)\n",
        "\n",
        "\n",
        "        else:\n",
        "            ids = llm.generate(\n",
        "                inputs_embeds=proj_h,\n",
        "                input_ids=torch.full(\n",
        "                  (1, proj_h.size(1)), fill_value=llm_tok.bos_token_id, device=DEVICE\n",
        "                ),\n",
        "                attention_mask=mask,\n",
        "                max_new_tokens=max_new,\n",
        "                eos_token_id=llm_tok.eos_token_id,\n",
        "                pad_token_id=llm_tok.pad_token_id,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "    return llm_tok.decode(ids[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Quantum computers promise speed-ups that classical machines cannot match.\",\n",
        "    \"The Aurora Borealis shimmered like green silk across the night sky.\",\n",
        "    \"After twelve grueling innings, the underdogs finally clinched the pennant.\",\n",
        "    \"Please email the revised PDF to marketing by 09:30 a.m. tomorrow.\",\n",
        "    \"Serendipity often favors the curious, the prepared, and the persistent.\",\n",
        "    \"“We’re out of espresso beans,” whispered the barista, glancing at the queue.\",\n",
        "    \"A tiny gecko clung effortlessly to the laboratory’s glass wall.\",\n",
        "    \"GDP growth slowed to 2.1 percent in Q2, missing analysts’ expectations.\",\n",
        "    \"Einstein’s handwriting was notoriously hard to read, even for his assistants.\",\n",
        "    \"The ancient oak, felled by lightning, revealed concentric rings of drought.\",\n",
        "    \"Machine-learning models excel at interpolation but struggle with extrapolation.\",\n",
        "    \"During the blackout, neighbors gathered in the hallway and shared candles.\",\n",
        "    \"My passport expires in eleven months; I’d better renew it soon.\",\n",
        "    \"Some mushrooms glow faintly in the dark due to bioluminescent enzymes.\",\n",
        "    \"Arctic sea-ice extent hit a record low, alarming climate scientists worldwide.\",\n",
        "    \"He typed rm -rf /*, realized the mistake, and pulled the network cable.\",\n",
        "    \"To brew oolong correctly, let the kettle cool to eighty-five degrees Celsius.\",\n",
        "    \"The violin’s G string snapped mid-concerto, yet the soloist played on.\",\n",
        "    \"Cicadas emerge every seventeen years, blanketing trees in vibrating sound.\",\n",
        "    \"Static friction exceeds kinetic friction—hence the sudden jerk when boxes slide.\",\n",
        "    \"In 2029 NASA aims to redirect a small asteroid, proving planetary-defense tech.\",\n",
        "    \"Her résumé boasted fluency in Sinhala, Japanese, and American Sign Language.\",\n",
        "    \"The bookstore’s resident cat sleeps on whatever stack you’re trying to reach.\",\n",
        "    \"Deep-sea vents host microbes that survive without sunlight, using chemosynthesis.\",\n",
        "    \"“Ctrl + Z is my best friend,” the developer joked after a bad deploy.\",\n",
        "    \"Sourdough starters are tiny ecosystems of yeast, lactobacilli, and hype.\",\n",
        "    \"The committee postponed the vote until October 3rd, citing budget ambiguities.\",\n",
        "    \"Long-form journalism thrives when readers trust slow, meticulous storytelling.\",\n",
        "    \"A single photon triggered the avalanche photodiode, logging a digital ‘1’.\",\n",
        "    \"The chef garnished the ramen with nori, scallions, and a seven-minute egg.\",\n",
        "    \"For centuries, cartographers added mythical islands to fill white space.\",\n",
        "    \"The smartwatch recorded 10,438 steps, but she still felt sedentary.\",\n",
        "    \"PyTorch 2.3 introduced compile(), merging graph-mode speed with eager simplicity.\",\n",
        "    \"Call me old-fashioned, but I still sync MP3s instead of streaming music.\",\n",
        "    \"Two koalas dozed in the eucalyptus, oblivious to the drone overhead.\",\n",
        "    \"His haiku read: autumn wind / carries lost umbrellas / into gray rivers.\",\n",
        "    \"The conference Wi-Fi password was taped, in Comic Sans, to every chair.\",\n",
        "    \"Riemann’s hypothesis remains unproven, yet primes obey its silent rhythm.\",\n",
        "    \"“No spoilers!” she yelled, scrolling past the season-finale tweets.\",\n",
        "    \"Solar panels blanketed the barn roof, outshining the rusty weather vane.\",\n",
        "    \"A cashierless store feels eerie until you realize how fast you can leave.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "v23eD0s6zQYS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------- Generation Test ------------------------------\n",
        "for sentence in test_sentences:\n",
        "  print(\"Input sentence (Ground Truth):   \", sentence)\n",
        "  print(f\"'Generated:  ', {neo2llama(sentence, max_new=0)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZnQyFoEzI51",
        "outputId": "d7e833e9-3745-4586-cca5-d354c8b5e431"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence (Ground Truth):    Quantum computers promise speed-ups that classical machines cannot match.\n",
            "'Generated:  ', Theon Comput promise speed of ups that that machines cannot..\n",
            "\n",
            "Input sentence (Ground Truth):    The Aurora Borealis shimmered like green silk across the night sky.\n",
            "'Generated:  ', The skyal ofalis themermerled like likekkk night night sky\n",
            "\n",
            "Input sentence (Ground Truth):    After twelve grueling innings, the underdogs finally clinched the pennant.\n",
            "'Generated:  ', The areteenteenlingingsies, the underubs finally finally finally the the.\n",
            "\n",
            "Input sentence (Ground Truth):    Please email the revised PDF to marketing by 09:30 a.m. tomorrow.\n",
            "'Generated:  ', The submit your newed PDF PDFetingeting 90::0....\n",
            "\n",
            "Input sentence (Ground Truth):    Serendipity often favors the curious, the prepared, and the persistent.\n",
            "'Generated:  ', The areitions,. leadss.. the curious. and,.ive\n",
            "\n",
            "Input sentence (Ground Truth):    “We’re out of espresso beans,” whispered the barista, glancing at the queue.\n",
            "'Generated:  ', TheI’re out of coffeepresso coffeeans” Iushed the theant, lookinginging the queue.\n",
            "\n",
            "Input sentence (Ground Truth):    A tiny gecko clung effortlessly to the laboratory’s glass wall.\n",
            "'Generated:  ', The small littleardpeocked to the the laboratory labor glass glass.\n",
            "\n",
            "Input sentence (Ground Truth):    GDP growth slowed to 2.1 percent in Q2, missing analysts’ expectations.\n",
            "'Generated:  ', TheDP growth slow to .. percent percentage quarter quarter2,, analystyst expect\n",
            "\n",
            "Input sentence (Ground Truth):    Einstein’s handwriting was notoriously hard to read, even for his assistants.\n",
            "'Generated:  ', Thestein’swriting was wasously a a to to, even even for assist assist\n",
            "\n",
            "Input sentence (Ground Truth):    The ancient oak, felled by lightning, revealed concentric rings of drought.\n",
            "'Generated:  ', The ancient city tree buried,, by light,, the the circular rings of of.\n",
            "\n",
            "Input sentence (Ground Truth):    Machine-learning models excel at interpolation but struggle with extrapolation.\n",
            "'Generated:  ', The learninglearning learning are in their interationalation but struggle with Ext-\n",
            "\n",
            "Input sentence (Ground Truth):    During the blackout, neighbors gathered in the hallway and shared candles.\n",
            "'Generated:  ', The the power,, neighb gathered gathered the hall hall hall and cand cand\n",
            "\n",
            "Input sentence (Ground Truth):    My passport expires in eleven months; I’d better renew it soon.\n",
            "'Generated:  ', The pass is isiress a years. I’s be give my pass.\n",
            "\n",
            "Input sentence (Ground Truth):    Some mushrooms glow faintly in the dark due to bioluminescent enzymes.\n",
            "'Generated:  ', The rareroomsle glelyly in the in due bi biloriccent\n",
            "\n",
            "Input sentence (Ground Truth):    Arctic sea-ice extent hit a record low, alarming climate scientists worldwide.\n",
            "'Generated:  ', Thectic Sea-iceice size hit hit low,, at climate scient scient world world\n",
            "\n",
            "Input sentence (Ground Truth):    He typed rm -rf /*, realized the mistake, and pulled the network cable.\n",
            "'Generated:  ', The pressed \"R-R/*, realized the mistake, typeded my network cable\n",
            "\n",
            "Input sentence (Ground Truth):    To brew oolong correctly, let the kettle cool to eighty-five degrees Celsius.\n",
            "'Generated:  ', The makew to to to, let let tea let cool theteenteenfivefive degrees degrees C..\n",
            "\n",
            "Input sentence (Ground Truth):    The violin’s G string snapped mid-concerto, yet the soloist played on.\n",
            "'Generated:  ', The violin’s G G Gappeded mid mid piano,,,, soloist\n",
            "\n",
            "Input sentence (Ground Truth):    Cicadas emerge every seventeen years, blanketing trees in vibrating sound.\n",
            "'Generated:  ', Theilac are from teen years,,inging trees in in v sound\n",
            "\n",
            "Input sentence (Ground Truth):    Static friction exceeds kinetic friction—hence the sudden jerk when boxes slide.\n",
            "'Generated:  ', Theicic exceed exceed exceed Mechan mechanical — — — suddenlykk when when\n",
            "\n",
            "Input sentence (Ground Truth):    In 2029 NASA aims to redirect a small asteroid, proving planetary-defense tech.\n",
            "'Generated:  ', The 229 NASA NASA to to toraft a a small astero,,, planet planet-\n",
            "\n",
            "Input sentence (Ground Truth):    Her résumé boasted fluency in Sinhala, Japanese, and American Sign Language.\n",
            "'Generated:  ', The backgroundume distinguished in in in inal,,,, American American sign..\n",
            "\n",
            "Input sentence (Ground Truth):    The bookstore’s resident cat sleeps on whatever stack you’re trying to reach.\n",
            "'Generated:  ', The book Book’s resident cat cat sle on whatever stack stack yourere to to.\n",
            "\n",
            "Input sentence (Ground Truth):    Deep-sea vents host microbes that survive without sunlight, using chemosynthesis.\n",
            "'Generated:  ', The Oceanear Seaans host microiteses surv surv surv without sun,, using using ch.\n",
            "\n",
            "Input sentence (Ground Truth):    “Ctrl + Z is my best friend,” the developer joked after a bad deploy.\n",
            "'Generated:  ', TheI’ is+” a friend friend,” says I the in a a a error deployment\n",
            "\n",
            "Input sentence (Ground Truth):    Sourdough starters are tiny ecosystems of yeast, lactobacilli, and hype.\n",
            "'Generated:  ', Therain,,akers are are e e e of of,,,,ob,,, and and\n",
            "\n",
            "Input sentence (Ground Truth):    The committee postponed the vote until October 3rd, citing budget ambiguities.\n",
            "'Generated:  ', The process iseded the the until October  ,, budgetinging confusion confusion\n",
            "\n",
            "Input sentence (Ground Truth):    Long-form journalism thrives when readers trust slow, meticulous storytelling.\n",
            "'Generated:  ', The-term writing journalss when readers trust trust,,-icellingelling\n",
            "\n",
            "Input sentence (Ground Truth):    A single photon triggered the avalanche photodiode, logging a digital ‘1’.\n",
            "'Generated:  ', The signal photon triggered thevalicic-,,, a digital1111\n",
            "\n",
            "Input sentence (Ground Truth):    The chef garnished the ramen with nori, scallions, and a seven-minute egg.\n",
            "'Generated:  ', The signaturef gar the the reng with withuki,,,,,, a a seven minute egg.\n",
            "\n",
            "Input sentence (Ground Truth):    For centuries, cartographers added mythical islands to fill white space.\n",
            "'Generated:  ', The are,, cartographers added myth islands islands to fill white space.\n",
            "\n",
            "Input sentence (Ground Truth):    The smartwatch recorded 10,438 steps, but she still felt sedentary.\n",
            "'Generated:  ', The Fwatch on 11,,  steps steps but she felt still feltic..\n",
            "\n",
            "Input sentence (Ground Truth):    PyTorch 2.3 introduced compile(), merging graph-mode speed with eager simplicity.\n",
            "'Generated:  ', The.y is is .3 introduced... toiling graph and for with eager\n",
            "\n",
            "Input sentence (Ground Truth):    Call me old-fashioned, but I still sync MP3s instead of streaming music.\n",
            "'Generated:  ', The it theschoolschoolist, I still prefer syncpsunes instead of streaming music\n",
            "\n",
            "Input sentence (Ground Truth):    Two koalas dozed in the eucalyptus, oblivious to the drone overhead.\n",
            "'Generated:  ', The dogsalals areows on the trees euery,,ign un a the dr\n",
            "\n",
            "Input sentence (Ground Truth):    His haiku read: autumn wind / carries lost umbrellas / into gray rivers.\n",
            "'Generated:  ', The poemai is read:umn wind / car Lost carried um umco the into into into\n",
            "\n",
            "Input sentence (Ground Truth):    The conference Wi-Fi password was taped, in Comic Sans, to every chair.\n",
            "'Generated:  ', The conference was Wi was was was captured ted,icicic, to every everych\n",
            "\n",
            "Input sentence (Ground Truth):    Riemann’s hypothesis remains unproven, yet primes obey its silent rhythm.\n",
            "'Generated:  ', Theung s cubent remains un unified,, prime remain subs its its silent.\n",
            "\n",
            "Input sentence (Ground Truth):    “No spoilers!” she yelled, scrolling past the season-finale tweets.\n",
            "'Generated:  ', TheI moresers!” I tweleded scrolling scrolling the season- finale finale Twitterets.\n",
            "\n",
            "Input sentence (Ground Truth):    Solar panels blanketed the barn roof, outshining the rusty weather vane.\n",
            "'Generated:  ', Thearels coveredaped the bar roof roof,,ing the theagedaged v.\n",
            "\n",
            "Input sentence (Ground Truth):    A cashierless store feels eerie until you realize how fast you can leave.\n",
            "'Generated:  ', The v terminal that store is feels eer until until realize realize how you you can..\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/drive/MyDrive/fMRIwork\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Timestamp versioning\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "filename = f\"generation_test_{timestamp}.txt\"\n",
        "filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "# Save generation test and config\n",
        "with open(filepath, 'w') as f:\n",
        "    f.write(\"# ------------------ Model Configuration ------------------\\n\")\n",
        "    f.write(f\"NEOBERT_CKPT  = {NEOBERT_CKPT}\\n\")\n",
        "    f.write(f\"LLAMA_CKPT    = {LLAMA_CKPT}\\n\")\n",
        "    f.write(f\"BATCH_SIZE    = {BATCH_SIZE}\\n\")\n",
        "    f.write(f\"LR            = {LR}\\n\")\n",
        "    f.write(f\"NUM_EPOCHS    = {NUM_EPOCHS}\\n\")\n",
        "    f.write(f\"MAX_LEN       = {MAX_LEN}\\n\")\n",
        "    f.write(f\"DEVICE        = {DEVICE}\\n\")\n",
        "    f.write(f\"SEED          = {SEED}\\n\")\n",
        "    f.write(\"\\n# ------------------ Generation Test Output ------------------\\n\")\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        output = neo2llama(sentence, max_new=0)\n",
        "        f.write(f\"Input sentence (Ground Truth):    {sentence}\\n\")\n",
        "        f.write(f\"Generated:   {output}\\n\\n\")\n",
        "\n",
        "print(f\"Saved generation test to: {filepath}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0RBu1cHgW0y",
        "outputId": "ab934316-b4c7-4407-d742-06435976da71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved generation test to: /content/drive/MyDrive/fMRIwork/generation_test_2025-06-17_04-03-48.txt\n"
          ]
        }
      ]
    }
  ]
}